{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Title\n",
    "Pietro Grandinetti,...\n",
    "\n",
    "Date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this article I am going to try out statistical inference techniques with the goal to draw insights from a pretty large dataset.\n",
    "\n",
    "I will touch on several subjects, from Python and PostgreSQL, to data sampling, hypothesis testing and inference, while driven by a business point of view. In other terms, I will put myself at the place of the Data Scientist who's been given the task to provide its business with data-driven, actionable insights.\n",
    "\n",
    "The content will thus be interesting for those of you who work on any of the following topics:\n",
    "- Python development\n",
    "- Database programming and optimization\n",
    "- Inferential Statistics\n",
    "- Data Science"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The data (and the story behind it)\n",
    "\n",
    "Raw data is the fuel behind the most important innovations of these days. It's a pity that many organizations keep their data secret. On the one hand, I understand their reasons; on the other, history has shown that open-source contributions drive innovation like, or even more than, competition.\n",
    "\n",
    "That's why we set on a mission to publish everything we do and make it reproducible. One of the first things we did was to take a complicated machinery developed by researchers at the Communication Systems Department\n",
    "of Sophia-Antipolis, France, that was already open-sourced and we made it more easily accessible and 100% reproducible.\n",
    "\n",
    "The result is... _a lot_ of data.\n",
    "\n",
    "This is a system that simulates road traffic in the Principality of Monaco for ten hours, starting from 4am. It basically is a fairly large network of roads (think of a graph) inside which vehicles of any type and pedestrians  move (think of objects moving from node to node of a graph).\n",
    "\n",
    "The key is that every vehicle is equipped with a GPS-like sensor. Therefore, at the end of the simulation, we can retrieve instantaneous positions and speed information for each vehicle. That sounds like a great dataset!\n",
    "\n",
    "We've done this already, put all data in CSV format as well as loaded them into a PostgreSQL database. Then (suprise!), we simply published the data on the internet. You can download all of them on your computer without even asking. And you can reproduce the experiment (or run your own) in a couple of clicks. Careful though, it's many gigabytes of data-- may take a while. I would recommend to first read this through [this article](url) that shows some interesting exploratory data analysis done over a subset of the data. **ADD LINK**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Settings\n",
    "\n",
    "The dataset is definitely great, though it presents an obvious obstacle to start with: it's a bit too large!\n",
    "\n",
    "In fact, I am not even sure how many records are there. For sure, it's fixed size, therefore I could theoretically download the CSV, wait for probably a few hours, then run a `wc -l` to know its size, wait several minutes and then I'd get the number. And then... what?\n",
    "\n",
    "I suspect the dataset contains about 100 million rows, but this information is useless. I also know it's a CSV of around 8 GB (I can see the file size in the browser when it asks me to confirm the download), but this information is not very useful either.\n",
    "\n",
    "I was not given a cluster of computers with a lot of memory. I am using my laptop with 8G of fast memory. Even if I download the entire file, how am I supposed to load it into memory?\n",
    "\n",
    "The same problem applies to the PostgreSQL database. For sure the database has a lot of advantages over the CSV file (and the unique disadvantage that you need to know a bit of SQL!), but it comes with problems too. First of all, to maintain it costs money. I was forbidden by my team to run a `select count(*)` which would consume a lot of memory and maybe even take down the CPU of the server that hosts the DB. In fact, I suspect that our database administrator has disabled queries like that one. Imagine if I were to take down the entire server!\n",
    "\n",
    "No reason to worry too much though. This is a fairly common situation for data scientists. Whether you were given 1 billion tweets, 500 million pictures, or 100 million payment transactions, you won't be able to analyze the dataset in its entirety.\n",
    "\n",
    "You and I need a hat."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose the right hat (Statistical Settings)\n",
    "\n",
    "I need to wear the statistician hat to work on this task.\n",
    "\n",
    "The database will be the _population_ that I have to study. I will have to come up with _hypothesis_ about this population, driven by _samples_ and _reject_ (or not) them via statistical _testing_ and _evidence_.\n",
    "\n",
    "Statisticians never assume to know the entire population. Think about healthcare studies: when a company wants to evaluate the effectiveness of a new drug, they certainly don't assume to know how the entire world's population would react to it. They take a sample (volunteers usually), test the drug on this small sample and then make conclusions based on statistical evidence.\n",
    "\n",
    "This is the correct Data Science approach for large dataset, and it's the one I'll use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My questions to the dataset\n",
    "\n",
    "Let's get down to business now.\n",
    "\n",
    "My approach to statistical analysis is to ask questions. These questions will drive sampling and light exploration, and then shape the hypotheses. Once I have some meaningful hypotheses, I will work on them statistically (via hypothesis testing).\n",
    "\n",
    "Here are my initial questions for this dataset.\n",
    "\n",
    "**What is the maximum capacity of the city?**\n",
    "\n",
    "In fact, I don't know if traffic gets ever so bad that the network reaches its capacity. Nobody told me that, and I suspect it's not the case.\n",
    "My question refers more to the maximum number of objects that are moving in the network _at the same time_. Again, I can't just scan the whole population to get the \"true\" answer, hence I will need to resort to some different technique and come up with a data-driven approximation of the \"true\" answer.\n",
    "\n",
    "**From where do most vehicles enter the network?**\n",
    "\n",
    "I think it can be very useful to present at my next team meeting a 2D map of the city that shows the hotspots where a lot of objects enter the city.\n",
    "\n",
    "**What are the most congested spots?**\n",
    "\n",
    "I am still thinking about a 2D map of the city. It would surely be useful to know where traffic gets the worst. I believe this can be interpreted in multiple ways: I could say that it's bad traffic if there a lot of cars, or if the speed is very low. This point will probably drive further analysis.\n",
    "\n",
    "**What is the highest speed vehicles travel at?**\n",
    "\n",
    "Again, not the \"true\" highest speed. Think statistically.\n",
    "\n",
    "**What is the average speed vehicles travel at?**\n",
    "\n",
    "Similar concept to the previous question.\n",
    "\n",
    "**Hypothesis: A vehicle is 95% likely to travel at an average speed of 20 km/h. True or False?**\n",
    "\n",
    "**What are the lengths of the 10 longest queues of vehicles? Where are they?**\n",
    "\n",
    "I believe the answer to this question will require some extra computational power."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"blue\">\n",
    "\n",
    "**How many vehicles uses the most taken roads?**\n",
    "\n",
    "**Does this value changes substantially WRT the traffic conditions?**\n",
    "\n",
    "Analysing this point may shows us whether congestions are mainy due to the higher demand if there can be some structural bottleneck in the network.\n",
    "\n",
    "**What percentage of their travel time do vehicles spend queueing?**\n",
    "\n",
    "Such ratio may give us an idea about the criticity of the congestions.\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries\n",
    "\n",
    "I am definitely going to need `pandas`. I will also need a connection to the database, and I will use `psycopg2` for managing it.\n",
    "Then, I will need `scipy` for the statistical part.\n",
    "\n",
    "I will of course need a few more modules and libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas\n",
    "!pip install psycopg2\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import psycopg2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I follow the best practice to have credentials and other environment variables in a `~/.env` file. Before executing this notebook, I do `$source .env` which is why now I can find them into `os.environ`. That's how I am going to initialize the database connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection = psycopg2.connect(\n",
    "    host=os.environ['DB_HOST'],\n",
    "    port=os.environ['DB_PORT'],\n",
    "    password=os.environ['DB_PASSWORD'],\n",
    "    user=os.environ['DB_USER'],\n",
    "    dbname=os.environ['DB_NAME']\n",
    ")\n",
    "assert connection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it for the preliminaries! Let me know go ahead and talk about the cornerstone of this article, _the sampling procedure_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling from the population"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Sampling_ literally means to extract a few elements from the population. The most famous example of it is probably the [Urn problem](https://en.wikipedia.org/wiki/Urn_problem).\n",
    "\n",
    "With respect to urns and health studies I am in a advantageous position. Here the population sits in a database therefore I can take as many samples as we want. There are however a few gotchas to keep in mind:\n",
    "\n",
    "- I shouldn't rely on the fact that \"it's a database so I can look at the whole\". In fact, I cannot. Not only because the size is very large, but rather because it's unknown. Somebody may put more data in there, at any time.\n",
    "- I have be careful on the load I put on the database CPU. Hundreds, or low thousands of records should be fine.\n",
    "- The samples have to be taken as randomly as possible. As you know, it's not easy to have real randomness in computer systems.\n",
    "\n",
    "Luckly, PostgreSQL is here for the rescue.\n",
    "\n",
    "From version 9.5, PostgreSQL supports an operator called [TABLESAMPLE](https://www.postgresql.org/docs/9.5/sql-select.html). Like the name suggests, this allows to take a sample of a table.\n",
    "\n",
    "Few, very important things to keep in mind:\n",
    "\n",
    "- `TABLESAMPLE` can be applied either with `SYSTEM` or with `BERNOULLI` as sampling method. The first one is _very_ fast, because it's based on blocks/pages, but because of this it's not really random. The latter is slower, but offer better randomization, and is the one I will use.\n",
    "- It requires a real number between 0 and 100, that represents the percentage of the table to sample. However, the result will not be the _exact_ percentage of rows, just a close number.\n",
    "- `TABLESAMPLE` is applied _before_ the `WHERE` clause. Hence, if I want to have 100 rows all respecting a certain condition, I have to sample more to try to get close to 100 after the `WHERE` is applied.\n",
    "\n",
    "This is a very brief overview and you should definitely read up the documentation for more details. There are some takeways for the experiments I will be working on.\n",
    "\n",
    "To start with, `TABLESAMPLE` can be _seeded_. This is common practice in numerical simulation, to initialize all pseudo-random algorithms with a seed, so that the experiment is reproducible. However, in this case I think it's unnecessary -- it doesn't fit well the inferential set-up, in my opinion. I will not seed the samples, hence keep in mind that when you run the notebook the results may vary slightly. **?TODO?**\n",
    "\n",
    "The second point I want to make is about the percentage used to select the sample. I want to underline that the percentage doesn't offer guarantee on the total number of records in the database, nor it does about the number of rows I will get in output! PostgreSQL _tries_ to select that percentage of rows, but for efficiency reasons it may actually be a different value. In other words, if I use 0.1 and I get back 1000 records, it doesn't mean that there are 10,000 records in total. In fact, if I run the same command again I may get a slightly different number."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before continuing, let's see `TABLESAMPLE` in action with `BERNOULLI` sampling method. I will ask PostgreSQL to give me 0.001% of the population of vehicle IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 112 ms, sys: 7.73 ms, total: 119 ms\n",
      "Wall time: 2min 8s\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 70 entries, 0 to 69\n",
      "Data columns (total 2 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   timestep_time  70 non-null     float64\n",
      " 1   vehicle_id     59 non-null     object \n",
      "dtypes: float64(1), object(1)\n",
      "memory usage: 1.2+ KB\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "    SELECT timestep_time\n",
    "           , vehicle_id\n",
    "    FROM most_0400_1300_1_5\n",
    "    TABLESAMPLE BERNOULLI (0.0001)\n",
    "\"\"\"\n",
    "df = %time pd.read_sql(query, connection)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I captured the execution time with the `%time` magic. As you can see, the _Wall time_, that is the time experienced by me waiting for output, is over 2 minutes. And this for fetching just ~70 records!\n",
    "\n",
    "Compare it with `TABLESAMPLE SYSTEM`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.98 ms, sys: 351 Âµs, total: 6.33 ms\n",
      "Wall time: 189 ms\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 70 entries, 0 to 69\n",
      "Data columns (total 2 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   timestep_time  70 non-null     float64\n",
      " 1   vehicle_id     69 non-null     object \n",
      "dtypes: float64(1), object(1)\n",
      "memory usage: 1.2+ KB\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "    SELECT timestep_time\n",
    "           , vehicle_id\n",
    "    FROM most_0400_1300_1_5\n",
    "    TABLESAMPLE SYSTEM (0.0001)\n",
    "\"\"\"\n",
    "df = %time pd.read_sql(query, connection)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case I received the result after just 189 ms. As I said, however, `SYSTEM` fetches pages of data therefore providing not-so-great of randomization.\n",
    "\n",
    "Now, finally, let's move on to the analsys!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
